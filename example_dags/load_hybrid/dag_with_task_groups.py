# Generated by InsightAir Framework - Task Groups Support Version 2.2
from pathlib import Path
from airflow import DAG
from airflow.utils.task_group import TaskGroup
from airflow.operators.python import BranchPythonOperator
from datetime import datetime, timedelta
from workflow_framework import framework
from workflow_framework.config import *
from workflow_framework import callbacks

# Workflow Configuration
workflow = 'na-policy-hist-db-edw-csv-to-text-load'
config_file = 'config_with_groups.yaml'
config_path = 'na/policy/history/tables/db_edw_csv_to_text/load_hybrid/'

# Initialize Framework
path = Path(__file__).with_name(config_file)
framework_instance = framework.Framework(workflow, config_path)

# Load configuration - properties from properties.yaml
config = Config(workflow, path)
config.load_configs()

# Extract properties from properties.yaml
properties = config.get_config().get('properties', {})
dag_config = config.load_config_light()

# DAG properties from properties.yaml
workflow_name = properties.get('workflow_name', workflow)
description = properties.get('workflow_description', 'Policy history text processing with task groups')
priority = properties.get('workflow_priority', 'P3')

# Parse configuration values
schedule_interval = properties.get('dag_schedule_interval', '@daily')
start_date_str = properties.get('dag_start_date', '2024-01-01')
start_date = datetime.strptime(start_date_str, '%Y-%m-%d')
catchup = properties.get('dag_catchup', False)
max_active_runs = properties.get('dag_max_active_runs', 1)
max_active_tasks = properties.get('dag_max_active_tasks', 10)
sla_mins = properties.get('sla_minutes', 30)
retries = properties.get('dag_retries', 3)
retry_delay_mins = properties.get('dag_retry_delay_minutes', 5)

# Tags and emails
tags_str = properties.get('dag_tags', 'TaskGroups,Policy,P3,Enhanced')
tags = [tag.strip() for tag in tags_str.split(',')]
ops_emails_str = properties.get('ops_emails', 'ops@company.com')
ops_emails = [email.strip() for email in ops_emails_str.split(',')]

def failure_callback(context):
    """Enhanced failure callback"""
    callbacks.failure_callback(context, config_path, workflow, priority)

def sla_miss_callback(context):
    """Enhanced SLA miss callback"""
    callbacks.sla_miss_callback(context, workflow, priority)

def success_callback(context):
    """Success callback"""
    print(f"Workflow {workflow} with task groups completed successfully at {datetime.now()}")

# Create the DAG with Task Groups Support
with DAG(
    dag_id=workflow_name,
    default_args={
        'owner': properties.get('dag_owner', 'insightair'),
        'depends_on_past': properties.get('dag_depends_on_past', False),
        'start_date': start_date,
        'retries': retries,
        'retry_delay': timedelta(minutes=retry_delay_mins),
        'email': ops_emails,
        'email_on_failure': properties.get('email_on_failure', True),
        'email_on_retry': properties.get('email_on_retry', False),
        'sla': timedelta(minutes=sla_mins),
    },
    description=description,
    schedule_interval=schedule_interval,
    start_date=start_date,
    catchup=catchup,
    on_failure_callback=failure_callback,
    on_success_callback=success_callback,
    sla_miss_callback=sla_miss_callback,
    tags=tags,
    max_active_runs=max_active_runs,
    max_active_tasks=max_active_tasks,
    params={
        'data_group': properties.get('workflow_data_group', 'production'),
        'environment': properties.get('environment', 'production'),
        'enable_task_groups': True,
        'emergency_mode': False
    }
) as dag:

    # ===== TASK GROUP CREATION FUNCTIONS =====
    
    def create_task_group(group_config, parent_dag=None):
        """
        Create a TaskGroup from configuration
        """
        group_name = group_config['name']
        group_type = group_config.get('type', 'TASK_GROUP')
        description = group_config.get('description', f'Task group: {group_name}')
        tooltip = group_config.get('tooltip', description)
        prefix_group_id = group_config.get('prefix_group_id', True)
        
        # Task group defaults
        defaults = dag_config.get('task_group_defaults', {})
        ui_color = group_config.get('ui_color', defaults.get('ui_color', '#f0f0f0'))
        ui_fgcolor = group_config.get('ui_fgcolor', defaults.get('ui_fgcolor', '#000000'))
        
        # Create TaskGroup
        task_group = TaskGroup(
            group_id=group_name,
            tooltip=tooltip,
            prefix_group_id=prefix_group_id,
            ui_color=ui_color,
            ui_fgcolor=ui_fgcolor,
            dag=dag
        )
        
        return task_group
    
    def create_tasks_in_group(tasks_config, task_group):
        """
        Create tasks within a task group
        """
        group_tasks = {}
        
        for task_config in tasks_config:
            task_name = task_config['name']
            task_type = task_config.get('type', 'DUMMY')
            
            # Handle nested task groups
            if task_type == 'TASK_GROUP':
                nested_group = create_task_group(task_config, dag)
                nested_tasks = create_tasks_in_group(task_config.get('tasks', []), nested_group)
                group_tasks[task_name] = nested_group
                group_tasks.update(nested_tasks)
                
                # Set up nested group dependencies
                setup_task_dependencies(task_config.get('tasks', []), nested_tasks, nested_group)
            else:
                # Create regular task within the group
                with task_group:
                    task = framework_instance.build_task(task_type, task_name)
                    task.doc_md = task_config.get('description', f'Task: {task_name}')
                    group_tasks[task_name] = task
        
        return group_tasks
    
    def setup_task_dependencies(tasks_config, task_dict, parent_group=None):
        """
        Set up task dependencies within a group
        """
        for task_config in tasks_config:
            task_name = task_config['name']
            parents = task_config.get('parents', [])
            
            if task_name in task_dict and parents:
                for parent_name in parents:
                    if parent_name in task_dict:
                        task_dict[parent_name] >> task_dict[task_name]
    
    # ===== TASK AND TASK GROUP CREATION =====
    
    # Store all tasks and groups for dependency setup
    all_tasks = {}
    all_groups = {}
    
    # Create tasks from configuration
    tasks_config = dag_config.get('tasks', [])
    
    for task_config in tasks_config:
        task_name = task_config['name']
        task_type = task_config.get('type', 'DUMMY')
        
        if task_type == 'TASK_GROUP':
            # Create task group
            task_group = create_task_group(task_config, dag)
            all_groups[task_name] = task_group
            
            # Create tasks within the group
            group_tasks = create_tasks_in_group(task_config.get('tasks', []), task_group)
            all_tasks.update(group_tasks)
            
            # Set up intra-group dependencies
            setup_task_dependencies(task_config.get('tasks', []), group_tasks, task_group)
            
        else:
            # Create regular task
            task = framework_instance.build_task(task_type, task_name)
            task.doc_md = task_config.get('description', f'Task: {task_name}')
            all_tasks[task_name] = task
    
    # ===== INTER-GROUP AND TASK DEPENDENCIES =====
    
    # Set up dependencies between tasks and groups
    for task_config in tasks_config:
        task_name = task_config['name']
        parents = task_config.get('parents', [])
        
        # Get current task/group
        current_item = all_tasks.get(task_name) or all_groups.get(task_name)
        
        if current_item and parents:
            for parent_name in parents:
                parent_item = all_tasks.get(parent_name) or all_groups.get(parent_name)
                if parent_item:
                    parent_item >> current_item
    
    # ===== DYNAMIC AND CONDITIONAL TASK GROUPS =====
    
    def create_dynamic_region_tasks(**context):
        """
        Example: Create dynamic task groups for regions
        """
        regions = context['dag_run'].conf.get('regions', ['US', 'EU', 'APAC'])
        dynamic_tasks = []
        
        for region in regions:
            with TaskGroup(group_id=f'process_{region.lower()}', tooltip=f'Process {region} data') as region_group:
                extract_task = framework_instance.build_task('DB_TO_TEXT', f'extract_{region.lower()}')
                validate_task = framework_instance.build_task('DQ', f'validate_{region.lower()}')
                
                extract_task >> validate_task
                dynamic_tasks.append(region_group)
        
        return dynamic_tasks
    
    def decide_processing_branch(**context):
        """
        Example: Conditional branching for emergency processing
        """
        emergency_mode = context['dag_run'].conf.get('emergency_mode', False)
        if emergency_mode:
            return 'emergency_processing_group'
        else:
            return 'data_processing_group'
    
    # ===== CONDITIONAL TASK GROUPS EXAMPLE =====
    
    # Emergency processing group (conditional)
    emergency_condition = "{{ dag_run.conf.get('emergency_mode', False) }}"
    
    with TaskGroup(
        group_id='emergency_processing_group',
        tooltip='Emergency processing tasks',
        prefix_group_id=True
    ) as emergency_group:
        priority_extract = framework_instance.build_task('DB_TO_TEXT', 'priority_extraction')
        priority_notify = framework_instance.build_task('EMAIL', 'priority_notification')
        
        priority_extract >> priority_notify
    
    # Branch operator for conditional execution
    branch_decision = BranchPythonOperator(
        task_id='decide_processing_path',
        python_callable=decide_processing_branch,
        dag=dag
    )
    
    # ===== TASK GROUP DOCUMENTATION =====
    
    # Add comprehensive documentation to task groups
    if 'data_validation_group' in all_groups:
        all_groups['data_validation_group'].doc_md = """
        ## Data Validation Group
        
        This task group performs comprehensive data validation before processing:
        
        - **validate_source**: Checks database connectivity and data availability
        - **check_dependencies**: Validates upstream data dependencies  
        - **validate_schema**: Ensures database schema compatibility
        - **validation_summary**: Generates validation summary report
        
        **Configuration**: Each task loads properties from separate YAML files
        **Runtime Override**: All validation thresholds can be overridden at runtime
        """
    
    if 'data_processing_group' in all_groups:
        all_groups['data_processing_group'].doc_md = """
        ## Data Processing Group (with Nested Groups)
        
        Main data processing pipeline with nested task groups:
        
        ### Extraction Subgroup
        - Policy data extraction and metadata collection
        - Parallel processing with merge operation
        
        ### Transformation Subgroup  
        - Business rule application
        - Data enrichment and formatting
        
        **Benefits**: Logical grouping, better UI organization, parallel execution
        """
    
    if 'quality_assurance_group' in all_groups:
        all_groups['quality_assurance_group'].doc_md = """
        ## Quality Assurance Group (Parallel Processing)
        
        Comprehensive quality validation with parallel execution:
        
        - **quality_check**: Data quality validation
        - **compliance_check**: Regulatory compliance verification
        - **performance_check**: Processing performance validation  
        - **security_check**: Security requirements validation
        - **qa_summary**: Consolidated QA report
        
        **Pattern**: All checks run in parallel, then results are merged
        """

# ===== USAGE EXAMPLES AND PATTERNS =====
"""
Task Group Configuration Patterns:

1. **Basic Task Group**:
   - name: "processing_group"
     type: "TASK_GROUP"
     tasks: [...]

2. **Nested Task Groups**:
   - name: "main_group"
     type: "TASK_GROUP"
     tasks:
       - name: "sub_group"
         type: "TASK_GROUP"
         tasks: [...]

3. **Parallel Task Groups**:
   Multiple groups with same parents run in parallel

4. **Conditional Task Groups**:
   Groups that execute based on runtime conditions

5. **Dynamic Task Groups**:
   Groups created programmatically based on runtime parameters

Runtime Parameters for Task Groups:
- enable_task_groups: true/false
- emergency_mode: true/false (activates emergency processing)
- regions: ["US", "EU"] (for dynamic region processing)
- group_parallelism: 5 (max parallel groups)
"""