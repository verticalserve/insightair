# Generated by InsightAir Framework - Hybrid Configuration Version 2.1
from pathlib import Path
from airflow import DAG
from datetime import datetime, timedelta
from workflow_framework import framework
from workflow_framework.config import *
from workflow_framework import callbacks

# Workflow Configuration
workflow = 'na-policy-hist-db-edw-csv-to-text-load'
config_file = 'config.yaml'
config_path = 'na/policy/history/tables/db_edw_csv_to_text/load_hybrid/'

# Initialize Framework
path = Path(__file__).with_name(config_file)
framework_instance = framework.Framework(workflow, config_path)

# Load configuration - properties now centralized in properties.yaml
config = Config(workflow, path)
config.load_configs()  # This loads both config.yaml and properties.yaml

# Extract properties from properties.yaml (all in flat structure for easy override)
properties = config.get_config().get('properties', {})

# DAG properties from properties.yaml
workflow_name = properties.get('workflow_name', workflow)
description = properties.get('workflow_description', 'Policy history text processing workflow')
priority = properties.get('workflow_priority', 'P3')

# DAG configuration from properties.yaml
schedule_interval = properties.get('dag_schedule_interval', '@daily')
start_date_str = properties.get('dag_start_date', '2024-01-01')
catchup = properties.get('dag_catchup', False)
max_active_runs = properties.get('dag_max_active_runs', 1)
max_active_tasks = properties.get('dag_max_active_tasks', 10)
owner = properties.get('dag_owner', 'insightair')
depends_on_past = properties.get('dag_depends_on_past', False)

# SLA and notification properties from properties.yaml
sla_mins = properties.get('sla_minutes', 30)
retries = properties.get('dag_retries', 3)
retry_delay_mins = properties.get('dag_retry_delay_minutes', 5)
email_on_failure = properties.get('email_on_failure', True)
email_on_retry = properties.get('email_on_retry', False)

# Tags and emails from properties.yaml
tags_str = properties.get('dag_tags', 'Policy,P3,TextProcessing')
tags = [tag.strip() for tag in tags_str.split(',')]
ops_emails_str = properties.get('ops_emails', 'ops@company.com')
ops_emails = [email.strip() for email in ops_emails_str.split(',')]

def failure_callback(context):
    """Enhanced failure callback with runtime parameter support"""
    callbacks.failure_callback(context, config_path, workflow, priority)

def sla_miss_callback(context):
    """Enhanced SLA miss callback"""
    callbacks.sla_miss_callback(context, workflow, priority)

def success_callback(context):
    """Success callback for workflow completion tracking"""
    print(f"Workflow {workflow} completed successfully at {datetime.now()}")

# Parse start date
start_date = datetime.strptime(start_date_str, '%Y-%m-%d')

# Create the Hybrid DAG with ALL properties from properties.yaml
with DAG(
    dag_id=workflow_name,
    default_args={
        'owner': owner,
        'depends_on_past': depends_on_past,
        'start_date': start_date,
        'retries': retries,
        'retry_delay': timedelta(minutes=retry_delay_mins),
        'email': ops_emails,
        'email_on_failure': email_on_failure,
        'email_on_retry': email_on_retry,
        'sla': timedelta(minutes=sla_mins),
    },
    description=description,
    schedule_interval=schedule_interval,
    start_date=start_date,
    catchup=catchup,
    on_failure_callback=failure_callback,
    on_success_callback=success_callback,
    sla_miss_callback=sla_miss_callback,
    tags=tags,
    max_active_runs=max_active_runs,
    max_active_tasks=max_active_tasks,
    params={
        # Default parameters that can be overridden at runtime
        'data_group': properties.get('workflow_data_group', 'production'),
        'environment': properties.get('environment', 'production'),
        'batch_size': properties.get('batch_size', 1000),
        'timeout_minutes': properties.get('timeout_minutes', 45),
        'enable_quality_checks': properties.get('quality_enable_checks', True),
        'parallel_processing': properties.get('enable_parallel_processing', True),
        'notification_level': 'standard'
    }
) as dag:

    # ===== RUNTIME PARAMETER SUPPORT =====
    # These functions support runtime parameter overrides
    
    def get_runtime_property(key, default_value, context=None):
        """
        Get property value with runtime override support
        Priority: DAG Run Conf > Airflow Variables > Default Properties > Default Value
        """
        from airflow.models import Variable
        
        # 1. Check DAG run configuration (highest priority)
        if context and 'dag_run' in context and context['dag_run'].conf:
            if key in context['dag_run'].conf:
                return context['dag_run'].conf[key]
        
        # 2. Check Airflow Variables
        try:
            var_value = Variable.get(key, default_var=None)
            if var_value is not None:
                return var_value
        except:
            pass
            
        # 3. Fall back to default value
        return default_value

    # ===== CUSTOMIZABLE TASK CHAIN =====
    # Users can modify this section to add/remove/reorder tasks
    # Task properties are loaded from separate files for maintainability
    
    # Task 1: Initialize workflow
    start = framework_instance.build_task('START', 'start')
    start.doc_md = """
    **Start Task - Runtime Parameter Support**
    
    Initializes the policy history text processing workflow with runtime parameter support.
    
    **Supported Runtime Parameters:**
    - `batch_size`: Number of records per batch (default: 1000)
    - `timeout_minutes`: Task timeout in minutes (default: 45)  
    - `enable_quality_checks`: Enable/disable quality validation (default: true)
    - `parallel_processing`: Enable parallel processing (default: true)
    - `environment`: Target environment (default: production)
    
    **Override Methods:**
    1. DAG Run Configuration: `{"batch_size": 2000}`
    2. Airflow Variables: Set variable `batch_size` = `2000`
    3. Environment Variables: `BATCH_SIZE=2000`
    """
    
    # Task 2: Validate source database (loads properties from validate_source.yaml)
    validate_source = framework_instance.build_task('DB', 'validate_source')
    validate_source.doc_md = """
    **Source Validation - Separate Properties File**
    
    **Properties File:** `validate_source.yaml`
    - Contains validation queries and rules
    - Runtime parameters: `source_connection_id`, `validation_timeout`
    - Task operator loads properties at runtime for flexibility
    
    **Runtime Overrides Supported:**
    - Connection ID can be overridden for different environments
    - Validation thresholds can be adjusted
    - Timeout values can be modified
    """
    
    # Task 3: Extract and convert policy data (loads properties from extract_policy_data.yaml)
    extract_policy_data = framework_instance.build_task('DB_TO_TEXT', 'extract_policy_data')
    extract_policy_data.doc_md = """
    **Policy Data Extraction - Complex Properties in Separate File**
    
    **Properties File:** `extract_policy_data.yaml`
    - Contains complex SQL queries (100+ lines)
    - Text formatting templates
    - Processing configuration
    - Performance tuning parameters
    
    **Why Separate File:**
    - Large SQL queries are easier to maintain
    - Text templates can be complex
    - Task operator loads properties on-demand
    - Better version control for query changes
    
    **Runtime Overrides:**
    - `batch_size`: Adjustable batch processing size
    - `output_path`: Can redirect output location
    - `compression`: Enable/disable compression
    """
    
    # Task 4: Data quality validation (loads properties from quality_check.yaml)
    quality_check = framework_instance.build_task('DQ', 'quality_check')
    quality_check.doc_md = """
    **Data Quality Validation - Complex Rules in Separate File**
    
    **Properties File:** `quality_check.yaml`
    - Contains detailed quality check definitions
    - Business rule validations
    - Quality metrics collection
    - Complex validation scripts
    
    **Runtime Overrides:**
    - `quality_threshold`: Adjustable quality thresholds
    - `enable_advanced_checks`: Toggle advanced validations
    - `sampling_percentage`: Adjust sampling for large datasets
    """
    
    # Task 5: Generate metadata (loads properties from create_metadata.yaml)
    create_metadata = framework_instance.build_task('SCRIPT', 'create_metadata')
    create_metadata.doc_md = """
    **Metadata Generation - Complex Script in Separate File**
    
    **Properties File:** `create_metadata.yaml`
    - Contains comprehensive Python script (200+ lines)
    - Business metrics calculations
    - Data lineage generation
    - Compliance metadata
    
    **Benefits of Separate File:**
    - Complex Python code is maintainable
    - Version control for script changes
    - Task loads script content at runtime
    - Easier testing and debugging
    """
    
    # Task 6: Archive data (loads properties from archive_source.yaml)
    archive_source = framework_instance.build_task('ARCHIVE', 'archive_source')
    archive_source.doc_md = """
    **Data Archival - Configuration in Separate File**
    
    **Properties File:** `archive_source.yaml`
    - Archive policies and retention rules
    - Compression and verification settings
    - Performance configurations
    
    **Runtime Overrides:**
    - `archive_retention_days`: Adjustable retention period
    - `compression_level`: Adjustable compression
    """
    
    # Task 7: Send notification (loads properties from send_notification.yaml)
    send_notification = framework_instance.build_task('EMAIL', 'send_notification')
    send_notification.doc_md = """
    **Completion Notification - Complex Template in Separate File**
    
    **Properties File:** `send_notification.yaml`
    - Contains HTML email template (300+ lines)
    - Business metrics formatting
    - Conditional recipient logic
    - Attachment configurations
    
    **Why Separate File:**
    - HTML templates are large and complex
    - Email formatting is easier to maintain
    - Template designers can work independently
    - Runtime parameter substitution in templates
    """
    
    # Task 8: Mark completion
    end = framework_instance.build_task('END', 'end')
    end.doc_md = """
    **End Task**
    
    Marks successful completion with runtime statistics logging.
    """

    # ===== CUSTOMIZABLE TASK FLOW DEFINITION =====
    # This section defines the task dependencies
    # Users can easily modify the flow based on requirements
    
    # Standard linear flow with validation
    start >> validate_source >> extract_policy_data >> quality_check
    
    # Parallel processing after quality check
    quality_check >> [create_metadata, archive_source]
    
    # Final notification and completion
    [create_metadata, archive_source] >> send_notification >> end
    
    # ===== ALTERNATIVE FLOW EXAMPLES =====
    # Users can uncomment and customize these flow patterns:
    
    # # Conditional processing based on runtime parameters
    # from airflow.operators.python import BranchPythonOperator
    # 
    # def choose_processing_path(**context):
    #     parallel_enabled = get_runtime_property('parallel_processing', True, context)
    #     return 'parallel_extract' if parallel_enabled else 'sequential_extract'
    # 
    # branch_task = BranchPythonOperator(
    #     task_id='choose_processing_path',
    #     python_callable=choose_processing_path,
    #     dag=dag
    # )
    # 
    # validate_source >> branch_task >> [parallel_extract, sequential_extract]
    
    # # Dynamic task generation based on runtime parameters
    # def create_dynamic_extraction_tasks(**context):
    #     batch_count = get_runtime_property('batch_count', 1, context)
    #     dynamic_tasks = []
    #     
    #     for i in range(batch_count):
    #         task = framework_instance.build_task('DB_TO_TEXT', f'extract_batch_{i}')
    #         dynamic_tasks.append(task)
    #     
    #     return dynamic_tasks
    
    # # Environment-specific task chains
    # def get_environment_specific_tasks(**context):
    #     env = get_runtime_property('environment', 'production', context)
    #     
    #     if env == 'development':
    #         return [validate_source, extract_policy_data, end]
    #     elif env == 'staging':
    #         return [validate_source, extract_policy_data, quality_check, end]
    #     else:  # production
    #         return [validate_source, extract_policy_data, quality_check, 
    #                create_metadata, archive_source, send_notification, end]

# ===== RUNTIME PARAMETER EXAMPLES =====
"""
Example runtime parameter overrides:

1. Via Airflow UI (DAG Run Configuration):
{
  "batch_size": 2000,
  "enable_quality_checks": false,
  "environment": "staging",
  "notification_level": "detailed"
}

2. Via Airflow Variables:
- Variable: batch_size, Value: 2000
- Variable: target_bucket, Value: custom-bucket-name
- Variable: timeout_minutes, Value: 60

3. Via CLI trigger:
airflow dags trigger na-policy-hist-db-edw-csv-to-text-load \
  --conf '{"batch_size": 2000, "parallel_processing": true}'

4. Via API:
POST /api/v1/dags/na-policy-hist-db-edw-csv-to-text-load/dagRuns
{
  "conf": {
    "batch_size": 2000,
    "environment": "staging"
  }
}
"""