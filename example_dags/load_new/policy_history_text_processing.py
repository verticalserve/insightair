# Generated by InsightAir Framework - Enhanced Version 2.0
from pathlib import Path
from airflow import DAG
from datetime import datetime, timedelta
from workflow_framework import framework
from workflow_framework.config import *
from workflow_framework import callbacks

# Workflow Configuration
workflow = 'na-policy-hist-db-edw-csv-to-text-load'
config_file = 'config.yaml'
config_path = 'na/policy/history/tables/db_edw_csv_to_text/load_new/'

# Initialize Framework
path = Path(__file__).with_name(config_file)
framework_instance = framework.Framework(workflow, config_path)

# Load configuration for DAG properties
config = Config(workflow, path)
dag_config = config.load_config_light()

# Extract DAG properties from enhanced config
metadata = dag_config.get('metadata', {})
properties = dag_config.get('properties', {})
notifications = properties.get('notifications', {})

# DAG Properties
ops_emails = notifications.get('ops_emails', ['ops@company.com'])
tags = ['Enhanced', 'Policy', metadata.get('priority', 'P3'), 'TextProcessing', 'Database']
sla_mins = notifications.get('sla_minutes', 30)
priority = metadata.get('priority', 'P3')
description = metadata.get('description', 'Policy history text processing workflow')

def failure_callback(context):
    """Enhanced failure callback with better error reporting"""
    callbacks.failure_callback(context, config_path, workflow, priority)

def sla_miss_callback(context):
    """Enhanced SLA miss callback"""
    callbacks.sla_miss_callback(context, workflow, priority)

def success_callback(context):
    """Success callback for workflow completion tracking"""
    print(f"Workflow {workflow} completed successfully at {datetime.now()}")

# Create the Enhanced DAG
with DAG(
    dag_id=workflow,
    default_args={
        'owner': 'insightair',
        'depends_on_past': False,
        'start_date': datetime(2024, 1, 1),
        'retries': properties.get('processing', {}).get('retry_count', 3),
        'retry_delay': timedelta(minutes=5),
        'email': ops_emails,
        'email_on_failure': notifications.get('alert_on_failure', True),
        'email_on_retry': False,
        'sla': timedelta(minutes=sla_mins),
    },
    description=description,
    schedule_interval='@daily',  # Can be overridden via Airflow variables
    start_date=datetime(2024, 1, 1),
    catchup=False,
    on_failure_callback=failure_callback,
    on_success_callback=success_callback,
    sla_miss_callback=sla_miss_callback,
    tags=tags,
    max_active_runs=1,
    max_active_tasks=10,
    params={
        'data_group': metadata.get('data_group', 'production'),
        'environment': 'production'
    }
) as dag:

    # ===== CUSTOMIZABLE TASK CHAIN =====
    # Users can modify this section to add/remove/reorder tasks
    # Each task is built using the framework with configuration from config.yaml
    
    # Task 1: Initialize workflow
    start = framework_instance.build_task('START', 'start')
    start.doc_md = """
    **Start Task**
    
    Initializes the policy history text processing workflow.
    - Sets up execution context
    - Validates environment variables
    - Prepares logging
    """
    
    # Task 2: Validate source database
    validate_source = framework_instance.build_task('DB', 'validate_source')
    validate_source.doc_md = """
    **Source Validation**
    
    Validates source database connectivity and data availability.
    - Checks database connection
    - Validates data freshness
    - Ensures minimum record count
    """
    
    # Task 3: Extract and convert policy data to text
    extract_policy_data = framework_instance.build_task('DB_TO_TEXT', 'extract_policy_data')
    extract_policy_data.doc_md = """
    **Policy Data Extraction**
    
    Main processing task that extracts policy history data and converts to text format.
    - Executes extraction query
    - Applies text template formatting
    - Outputs structured text files
    - Handles batching for large datasets
    """
    
    # Task 4: Data quality validation
    quality_check = framework_instance.build_task('DQ', 'quality_check')
    quality_check.doc_md = """
    **Data Quality Validation**
    
    Performs comprehensive quality checks on extracted text files.
    - Validates file count and sizes
    - Checks content patterns
    - Ensures data integrity
    """
    
    # Task 5: Generate processing metadata
    create_metadata = framework_instance.build_task('SCRIPT', 'create_metadata')
    create_metadata.doc_md = """
    **Metadata Generation**
    
    Creates comprehensive metadata for the processed data.
    - Generates processing statistics
    - Creates data lineage information
    - Stores execution metrics
    """
    
    # Task 6: Archive processed data
    archive_source = framework_instance.build_task('ARCHIVE', 'archive_source')
    archive_source.doc_md = """
    **Data Archival**
    
    Archives processed data for compliance and backup.
    - Moves data to archive location
    - Sets retention policies
    - Maintains data governance
    """
    
    # Task 7: Send completion notification
    send_notification = framework_instance.build_task('EMAIL', 'send_notification')
    send_notification.doc_md = """
    **Completion Notification**
    
    Sends success notification with processing summary.
    - Notifies operations team
    - Includes processing statistics
    - Provides data location information
    """
    
    # Task 8: Mark workflow completion
    end = framework_instance.build_task('END', 'end')
    end.doc_md = """
    **End Task**
    
    Marks successful completion of the workflow.
    - Finalizes execution context
    - Updates workflow status
    - Cleans up temporary resources
    """

    # ===== TASK FLOW DEFINITION =====
    # This section defines the task dependencies
    # Users can customize this flow based on their requirements
    
    # Linear flow with validation and quality checks
    start >> validate_source >> extract_policy_data >> quality_check
    
    # Parallel processing after quality check
    quality_check >> [create_metadata, archive_source]
    
    # Final notification and completion
    [create_metadata, archive_source] >> send_notification >> end
    
    # Alternative flows can be implemented:
    # 
    # Parallel processing example:
    # start >> [validate_source, setup_task] >> extract_policy_data
    # 
    # Conditional branching example:
    # quality_check >> branch_task >> [success_path, failure_path]
    # 
    # Multiple extraction tasks example:
    # validate_source >> [extract_task_1, extract_task_2] >> merge_task

# ===== DYNAMIC TASK GENERATION (Optional) =====
# Users can uncomment and customize this section for dynamic task creation

# def create_dynamic_tasks():
#     """
#     Example function to create tasks dynamically based on configuration
#     """
#     dynamic_tasks = []
#     
#     # Get dynamic configuration from config.yaml or external source
#     dynamic_config = dag_config.get('dynamic_tasks', [])
#     
#     for task_config in dynamic_config:
#         task = framework_instance.build_task(
#             task_config['type'], 
#             task_config['name']
#         )
#         dynamic_tasks.append(task)
#     
#     return dynamic_tasks

# dynamic_tasks = create_dynamic_tasks()
# if dynamic_tasks:
#     extract_policy_data >> dynamic_tasks >> quality_check